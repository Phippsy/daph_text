"0","library(rvest)"
"0","library(dplyr)"
"0","library(stringr)"
"0","library(tidytext)"
"0","library(ggplot2)"
"0","library(ggthemes)"
"0","library(tm)"
"0","library(tidyr)"
"0","library(wordcloud)"
"0","library(tm)"
"0","library(knitr)"
"0","library(tidyverse)"
"0","library(forcats)"
"0","library(lubridate)"
"0","# Get the highest page count from the site"
"0","max_pages <- read_html(""http://www.analyticshour.io/all-podcast-episodes/"") %>% "
"0","  html_nodes(""a.page-numbers"") %>% "
"0","  html_text() %>% "
"0","  as.numeric() %>% "
"0","  max(na.rm = TRUE)"
"0","page_nums <- as.list(1:max_pages)"
"0","# Function to get a list of all archive urls along with the episode number and year"
"0","get_url_list <- function(page_num) {"
"0","  x <- read_html(paste0(""http://www.analyticshour.io/all-podcast-episodes/page/"", page_num))"
"0","  # Make a DF containing the url, year and episode number"
"0","  urls <- data.frame( urls = x %>% "
"0","    html_nodes(paste0(""body > div.super-container.light-icons > div.main-content.page.archive-page > div > div > div > div  div > footer > ul > li.title.not-truncate "")) %>%"
"0","    html_children %>% "
"0","    as.character() %>% "
"0","    str_extract('\\\"".*?\\\""') %>% "
"0","    str_replace_all(c('\""' = ''))) %>% "
"0","    separate(urls, "
"0","             into = c(""bin"", ""bin2"", ""bin3"", ""year"", ""bin4"", ""bin5"", ""episode""),"
"0","             sep = ""\\/"","
"0","             remove = FALSE) %>% "
"0","    mutate(urls = as.character(urls), episode = str_replace_all(episode, ""\\-.*"", """")) %>% "
"0","    select(urls, year, episode)"
"0","    "
"0","    urls"
"0","}"
"0","# Pull the urls list from the site. Ignore the ""Too many values"" error, this comes from the call to separate()"
"0","full_urls <- map_df(page_nums, get_url_list) "
"0","# Split the url DF into a list so it can be called by map_df below"
"0","full_urls_split <- full_urls %>% "
"0","  split(.$episode)"
"0","# Function to scrape the web content per url, append the episode #, year and speaker name (where known) then tokenize into words"
"0","get_post_content <- function(ep_df) {"
"0","  # Quick line for debugging purposes"
"0","  # print(paste(""Running for :"", ep_df$url))"
"0","  url <- ep_df$url"
"0","  episode <- ep_df$episode"
"0","  year <- ep_df$year"
"0","  "
"0","  post_html <- read_html(url) "
"0","  "
"0","  post_ps <- post_html %>% "
"0","    html_nodes("".post p"") %>% "
"0","    html_text() %>% "
"0","    as.data.frame()"
"0","  "
"0","  post_title <- post_html %>% "
"0","    html_nodes("".entry-title"") %>% "
"0","    html_text() %>% "
"0","    as.character()"
"0","    "
"0","  names(post_ps) <- ""text"""
"0","  "
"0","  post_ps <- post_ps %>% "
"0","    dplyr::filter(str_detect(text, ""(\\d{2}:)"")) "
"0","  "
"0","  if (length(post_ps$text != 0)) {"
"0","    post_ps <- post_ps %>% "
"0","      mutate("
"0","        text = as.character(text),"
"0","        speaker = case_when("
"0","          str_detect(text, ""(MH:)|(Michael Helbling:)|(Michael:)"") ~ ""Drunk Helbs"","
"0","          str_detect(text, ""(MK:)|(Moe Kiss:)|(Moe:)"") ~ ""Moe from down under"","
"0","          str_detect(text, ""(TW:)|(Tim Wilson:)|(Tim:)"") ~ ""Grumpy Cat"","
"0","          TRUE ~ ""Unknown"""
"0","        ),"
"0","      url = url,"
"0","      episode = as.numeric(episode),"
"0","      year = year,"
"0","      title = post_title,"
"0","      time_start = as.numeric(ms(str_extract(text, ""\\d{2}:\\d{2}""))),"
"0","      time_end = lead(time_start, 1),"
"0","      duration = time_end - time_start"
"0","    )"
"0","    post_ps"
"0","  } else {"
"0","    NULL"
"0","  }"
"0","}"
"0","daph_post_text <- map_df(full_urls_split, get_post_content)      "
"0","saveRDS(daph_post_text, ""datafiles/daph_post_text.rds"")"
