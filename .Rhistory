remove = FALSE) %>%
mutate(urls = as.character(urls), episode = str_replace_all(episode, "\\-.*", "")) %>%
select(urls, year, episode)
urls
}
# Pull the urls list from the site. Ignore the "Too many values" error, this comes from the call to separate()
full_urls <- map_df(page_nums, get_url_list)
# Split the url DF into a list so it can be called by map_df below
full_urls_split <- full_urls %>%
split(.$episode)
# Function to scrape the web content per url, append the episode #, year and speaker name (where known) then tokenize into words
get_post_content <- function(ep_df) {
# Quick line for debugging purposes
# print(paste("Running for :", ep_df$url))
url <- ep_df$url
episode <- ep_df$episode
year <- ep_df$year
post_html <- read_html(url)
post_ps <- post_html %>%
html_nodes(".post p") %>%
html_text() %>%
as.data.frame()
post_title <- post_html %>%
html_nodes(".entry-title") %>%
html_text() %>%
as.character()
names(post_ps) <- "text"
post_ps <- post_ps %>%
dplyr::filter(str_detect(text, "(\\d{2}:)"))
if (length(post_ps$text != 0)) {
post_ps <- post_ps %>%
mutate(
text = as.character(text),
speaker = case_when(
str_detect(text, "(MH:)|(Michael Helbling:)") ~ "Drunk Helbs",
str_detect(text, "(MK:)|(Moe Kiss:)") ~ "Moe from down under",
str_detect(text, "(TW:)|(Tim Wilson:)") ~ "Grumpy Cat",
TRUE ~ "Unknown"
),
url = url,
episode = as.numeric(episode),
year = year,
title = post_title,
time_start = ms(str_extract(text, "\\d{2}:\\d{2}")),
time_end = lead(time_start, 1),
duration = time_end - time_start
) %>%
unnest_tokens(output = word, input = text, format = "text")
post_ps
} else {
NULL
}
}
daph_post_text <- map_df(full_urls_split, get_post_content)
data(stop_words)
daph_post_text <- daph_post_text %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "\\d{2}"))
View(daph_post_text)
url <- ep_df$url
episode <- ep_df$episode
year <- ep_df$year
post_html <- read_html(url)
url <- ep_df$url
episode <- ep_df$episode
year <- ep_df$year
post_html <- read_html(url)
post_ps <- post_html %>%
html_nodes(".post p") %>%
html_text() %>%
as.data.frame()
post_title <- post_html %>%
html_nodes(".entry-title") %>%
html_text() %>%
as.character()
names(post_ps) <- "text"
post_ps <- post_ps %>%
dplyr::filter(str_detect(text, "(\\d{2}:)"))
if (length(post_ps$text != 0)) {
post_ps <- post_ps %>%
mutate(
text = as.character(text),
speaker = case_when(
str_detect(text, "(MH:)|(Michael Helbling:)") ~ "Drunk Helbs",
str_detect(text, "(MK:)|(Moe Kiss:)") ~ "Moe from down under",
str_detect(text, "(TW:)|(Tim Wilson:)") ~ "Grumpy Cat",
TRUE ~ "Unknown"
),
url = url,
episode = as.numeric(episode),
year = year,
title = post_title,
time_start = ms(str_extract(text, "\\d{2}:\\d{2}")),
time_end = lead(time_start, 1),
duration = time_end - time_start
)
}
url <- ep_df$url
episode <- ep_df$episode
year <- ep_df$year
post_html <- read_html(url)
ep_df
post_ps <- post_html %>%
html_nodes(".post p") %>%
html_text() %>%
as.data.frame()
post_title <- post_html %>%
html_nodes(".entry-title") %>%
html_text() %>%
as.character()
names(post_ps) <- "text"
post_ps <- post_ps %>%
dplyr::filter(str_detect(text, "(\\d{2}:)"))
ep_df <- full_urls_split[[66]]
url <- ep_df$url
episode <- ep_df$episode
year <- ep_df$year
post_html <- read_html(url)
post_ps <- post_html %>%
html_nodes(".post p") %>%
html_text() %>%
as.data.frame()
post_title <- post_html %>%
html_nodes(".entry-title") %>%
html_text() %>%
as.character()
names(post_ps) <- "text"
post_ps <- post_ps %>%
dplyr::filter(str_detect(text, "(\\d{2}:)"))
post_ps <- post_ps %>%
mutate(
text = as.character(text),
speaker = case_when(
str_detect(text, "(MH:)|(Michael Helbling:)") ~ "Drunk Helbs",
str_detect(text, "(MK:)|(Moe Kiss:)") ~ "Moe from down under",
str_detect(text, "(TW:)|(Tim Wilson:)") ~ "Grumpy Cat",
TRUE ~ "Unknown"
),
url = url,
episode = as.numeric(episode),
year = year,
title = post_title,
time_start = ms(str_extract(text, "\\d{2}:\\d{2}")),
time_end = lead(time_start, 1),
duration = time_end - time_start
)
View(post_ps)
post_ps <- post_ps %>%
mutate(
text = as.character(text),
speaker = case_when(
str_detect(text, "(MH:)|(Michael Helbling:)") ~ "Drunk Helbs",
str_detect(text, "(MK:)|(Moe Kiss:)") ~ "Moe from down under",
str_detect(text, "(TW:)|(Tim Wilson:)") ~ "Grumpy Cat",
TRUE ~ "Unknown"
),
url = url,
episode = as.numeric(episode),
year = year,
title = post_title,
time_start = ms(str_extract(text, "\\d{2}:\\d{2}")),
time_end = lead(ms(str_extract(text, "\\d{2}:\\d{2}")), 1),
duration = time_end - time_start
)
post_ps <- post_ps %>%
mutate(
text = as.character(text),
speaker = case_when(
str_detect(text, "(MH:)|(Michael Helbling:)") ~ "Drunk Helbs",
str_detect(text, "(MK:)|(Moe Kiss:)") ~ "Moe from down under",
str_detect(text, "(TW:)|(Tim Wilson:)") ~ "Grumpy Cat",
TRUE ~ "Unknown"
),
url = url,
episode = as.numeric(episode),
year = year,
title = post_title,
time_start = ms(str_extract(text, "\\d{2}:\\d{2}")),
time_end = lead(ms(str_extract(text, "\\d{2}:\\d{2}")), 1),
duration = time_end - time_start
)
post_ps <- post_ps %>%
mutate(
text = as.character(text),
speaker = case_when(
str_detect(text, "(MH:)|(Michael Helbling:)") ~ "Drunk Helbs",
str_detect(text, "(MK:)|(Moe Kiss:)") ~ "Moe from down under",
str_detect(text, "(TW:)|(Tim Wilson:)") ~ "Grumpy Cat",
TRUE ~ "Unknown"
),
url = url,
episode = as.numeric(episode),
year = year,
title = post_title,
time_start = as.numeric(ms(str_extract(text, "\\d{2}:\\d{2}"))),
time_end = lead(time_start, 1),
duration = time_end - time_start
)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(rvest)
library(dplyr)
library(stringr)
library(tidytext)
library(ggplot2)
library(ggthemes)
library(tm)
library(tidyr)
library(wordcloud)
library(tm)
library(knitr)
library(tidyverse)
library(forcats)
library(lubridate)
# Get the highest page count from the site
max_pages <- read_html("http://www.analyticshour.io/all-podcast-episodes/") %>%
html_nodes("a.page-numbers") %>%
html_text() %>%
as.numeric() %>%
max(na.rm = TRUE)
page_nums <- as.list(1:max_pages)
# Function to get a list of all archive urls along with the episode number and year
get_url_list <- function(page_num) {
x <- read_html(paste0("http://www.analyticshour.io/all-podcast-episodes/page/", page_num))
# Make a DF containing the url, year and episode number
urls <- data.frame( urls = x %>%
html_nodes(paste0("body > div.super-container.light-icons > div.main-content.page.archive-page > div > div > div > div  div > footer > ul > li.title.not-truncate ")) %>%
html_children %>%
as.character() %>%
str_extract('\\\".*?\\\"') %>%
str_replace_all(c('\"' = ''))) %>%
separate(urls,
into = c("bin", "bin2", "bin3", "year", "bin4", "bin5", "episode"),
sep = "\\/",
remove = FALSE) %>%
mutate(urls = as.character(urls), episode = str_replace_all(episode, "\\-.*", "")) %>%
select(urls, year, episode)
urls
}
# Pull the urls list from the site. Ignore the "Too many values" error, this comes from the call to separate()
full_urls <- map_df(page_nums, get_url_list)
# Split the url DF into a list so it can be called by map_df below
full_urls_split <- full_urls %>%
split(.$episode)
# Function to scrape the web content per url, append the episode #, year and speaker name (where known) then tokenize into words
get_post_content <- function(ep_df) {
# Quick line for debugging purposes
# print(paste("Running for :", ep_df$url))
url <- ep_df$url
episode <- ep_df$episode
year <- ep_df$year
post_html <- read_html(url)
post_ps <- post_html %>%
html_nodes(".post p") %>%
html_text() %>%
as.data.frame()
post_title <- post_html %>%
html_nodes(".entry-title") %>%
html_text() %>%
as.character()
names(post_ps) <- "text"
post_ps <- post_ps %>%
dplyr::filter(str_detect(text, "(\\d{2}:)"))
if (length(post_ps$text != 0)) {
post_ps <- post_ps %>%
mutate(
text = as.character(text),
speaker = case_when(
str_detect(text, "(MH:)|(Michael Helbling:)") ~ "Drunk Helbs",
str_detect(text, "(MK:)|(Moe Kiss:)") ~ "Moe from down under",
str_detect(text, "(TW:)|(Tim Wilson:)") ~ "Grumpy Cat",
TRUE ~ "Unknown"
),
url = url,
episode = as.numeric(episode),
year = year,
title = post_title,
time_start = as.numeric(ms(str_extract(text, "\\d{2}:\\d{2}"))),
time_end = lead(time_start, 1),
duration = time_end - time_start
) %>%
unnest_tokens(output = word, input = text, format = "text")
post_ps
} else {
NULL
}
}
daph_post_text <- map_df(full_urls_split, get_post_content)
library(rvest)
library(dplyr)
library(stringr)
library(tidytext)
library(ggplot2)
library(ggthemes)
library(tm)
library(tidyr)
library(wordcloud)
library(tm)
library(knitr)
library(tidyverse)
library(forcats)
library(lubridate)
# Get the highest page count from the site
max_pages <- read_html("http://www.analyticshour.io/all-podcast-episodes/") %>%
html_nodes("a.page-numbers") %>%
html_text() %>%
as.numeric() %>%
max(na.rm = TRUE)
page_nums <- as.list(1:max_pages)
# Function to get a list of all archive urls along with the episode number and year
get_url_list <- function(page_num) {
x <- read_html(paste0("http://www.analyticshour.io/all-podcast-episodes/page/", page_num))
# Make a DF containing the url, year and episode number
urls <- data.frame( urls = x %>%
html_nodes(paste0("body > div.super-container.light-icons > div.main-content.page.archive-page > div > div > div > div  div > footer > ul > li.title.not-truncate ")) %>%
html_children %>%
as.character() %>%
str_extract('\\\".*?\\\"') %>%
str_replace_all(c('\"' = ''))) %>%
separate(urls,
into = c("bin", "bin2", "bin3", "year", "bin4", "bin5", "episode"),
sep = "\\/",
remove = FALSE) %>%
mutate(urls = as.character(urls), episode = str_replace_all(episode, "\\-.*", "")) %>%
select(urls, year, episode)
urls
}
# Pull the urls list from the site. Ignore the "Too many values" error, this comes from the call to separate()
full_urls <- map_df(page_nums, get_url_list)
# Split the url DF into a list so it can be called by map_df below
full_urls_split <- full_urls %>%
split(.$episode)
# Function to scrape the web content per url, append the episode #, year and speaker name (where known) then tokenize into words
get_post_content <- function(ep_df) {
# Quick line for debugging purposes
# print(paste("Running for :", ep_df$url))
url <- ep_df$url
episode <- ep_df$episode
year <- ep_df$year
post_html <- read_html(url)
post_ps <- post_html %>%
html_nodes(".post p") %>%
html_text() %>%
as.data.frame()
post_title <- post_html %>%
html_nodes(".entry-title") %>%
html_text() %>%
as.character()
names(post_ps) <- "text"
post_ps <- post_ps %>%
dplyr::filter(str_detect(text, "(\\d{2}:)"))
if (length(post_ps$text != 0)) {
post_ps <- post_ps %>%
mutate(
text = as.character(text),
speaker = case_when(
str_detect(text, "(MH:)|(Michael Helbling:)") ~ "Drunk Helbs",
str_detect(text, "(MK:)|(Moe Kiss:)") ~ "Moe from down under",
str_detect(text, "(TW:)|(Tim Wilson:)") ~ "Grumpy Cat",
TRUE ~ "Unknown"
),
url = url,
episode = as.numeric(episode),
year = year,
title = post_title,
time_start = as.numeric(ms(str_extract(text, "\\d{2}:\\d{2}"))),
time_end = lead(time_start, 1),
duration = time_end - time_start
) %>%
unnest_tokens(output = word, input = text, format = "text")
post_ps
} else {
NULL
}
}
daph_post_text <- map_df(full_urls_split, get_post_content)
data(stop_words)
daph_post_text <- daph_post_text %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "\\d{2}"))
View(daph_post_text)
library(rvest)
library(dplyr)
library(stringr)
library(tidytext)
library(ggplot2)
library(ggthemes)
library(tm)
library(tidyr)
library(wordcloud)
library(tm)
library(knitr)
library(tidyverse)
library(forcats)
library(lubridate)
# Get the highest page count from the site
max_pages <- read_html("http://www.analyticshour.io/all-podcast-episodes/") %>%
html_nodes("a.page-numbers") %>%
html_text() %>%
as.numeric() %>%
max(na.rm = TRUE)
page_nums <- as.list(1:max_pages)
# Function to get a list of all archive urls along with the episode number and year
get_url_list <- function(page_num) {
x <- read_html(paste0("http://www.analyticshour.io/all-podcast-episodes/page/", page_num))
# Make a DF containing the url, year and episode number
urls <- data.frame( urls = x %>%
html_nodes(paste0("body > div.super-container.light-icons > div.main-content.page.archive-page > div > div > div > div  div > footer > ul > li.title.not-truncate ")) %>%
html_children %>%
as.character() %>%
str_extract('\\\".*?\\\"') %>%
str_replace_all(c('\"' = ''))) %>%
separate(urls,
into = c("bin", "bin2", "bin3", "year", "bin4", "bin5", "episode"),
sep = "\\/",
remove = FALSE) %>%
mutate(urls = as.character(urls), episode = str_replace_all(episode, "\\-.*", "")) %>%
select(urls, year, episode)
urls
}
# Pull the urls list from the site. Ignore the "Too many values" error, this comes from the call to separate()
full_urls <- map_df(page_nums, get_url_list)
# Split the url DF into a list so it can be called by map_df below
full_urls_split <- full_urls %>%
split(.$episode)
# Function to scrape the web content per url, append the episode #, year and speaker name (where known) then tokenize into words
get_post_content <- function(ep_df) {
# Quick line for debugging purposes
# print(paste("Running for :", ep_df$url))
url <- ep_df$url
episode <- ep_df$episode
year <- ep_df$year
post_html <- read_html(url)
post_ps <- post_html %>%
html_nodes(".post p") %>%
html_text() %>%
as.data.frame()
post_title <- post_html %>%
html_nodes(".entry-title") %>%
html_text() %>%
as.character()
names(post_ps) <- "text"
post_ps <- post_ps %>%
dplyr::filter(str_detect(text, "(\\d{2}:)"))
if (length(post_ps$text != 0)) {
post_ps <- post_ps %>%
mutate(
text = as.character(text),
speaker = case_when(
str_detect(text, "(MH:)|(Michael Helbling:)") ~ "Drunk Helbs",
str_detect(text, "(MK:)|(Moe Kiss:)") ~ "Moe from down under",
str_detect(text, "(TW:)|(Tim Wilson:)") ~ "Grumpy Cat",
TRUE ~ "Unknown"
),
url = url,
episode = as.numeric(episode),
year = year,
title = post_title,
time_start = as.numeric(ms(str_extract(text, "\\d{2}:\\d{2}"))),
time_end = lead(time_start, 1),
duration = time_end - time_start
)
post_ps
} else {
NULL
}
}
daph_post_text <- map_df(full_urls_split, get_post_content)
data(stop_words)
daph_post_text <- daph_post_text %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "\\d{2}"))
daph_post_text %>%
arrange(desc(duration))
daph_post_text %>%
arrange(desc(duration))$text[1]
daph_post_text %>%
arrange(desc(duration))
daph_post_text %>%
arrange(desc(duration))$text
daph_post_text %>%
arrange(desc(duration))$text
daph_post_text %>%
arrange(desc(duration)) %>%
select(speaker, text) %>%
top_n(1, duration)
daph_post_text %>%
arrange(desc(duration)) %>%
top_n(1, duration) %>%
select(speaker, text)
monologue <- daph_post_text %>%
arrange(desc(duration)) %>%
top_n(1, duration) %>%
select(speaker, text)
monologue$text
View(daph_post_text)
monologue <- daph_post_text %>%
arrange(desc(duration)) %>%
top_n(1, duration) %>%
select(speaker, text, title)
monologue$text
View(monologue)
monologue <- daph_post_text %>%
arrange(desc(duration)) %>%
top_n(10, duration) %>%
select(speaker, text, title, duration)
monologue
monologue$text[1]
monologue$duration
mean(daph_post_text$duration)
mean(daph_post_text$duration, na.rm = TRUE)
install.packages('tidyverse')
install.packages('plotly')
